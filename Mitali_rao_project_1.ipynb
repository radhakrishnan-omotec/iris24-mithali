{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnhIzhzWUZEc7zNc2vmj7/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radhakrishnan-omotec/iris24-mithali/blob/main/Mitali_rao_project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Project Title**\n",
        "#The Impact of Music Genres on Study Efficiency: Analyzing Distraction through Eye and Facial Movement Tracking thru Biometrics\n",
        "\n",
        "### Author\n",
        "**Mitali Rao**\n"
      ],
      "metadata": {
        "id": "RibtoYog5gel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Environment Setup:\n",
        "\n",
        "    Install necessary libraries: Run the following command if needed:\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "sHt6pXeY5zJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python tensorflow librosa statsmodels scikit-learn"
      ],
      "metadata": {
        "id": "_68EH8be5uT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Python notebook Methodologuy outline:\n",
        "\n",
        "Implement in Google Colab for your project titled \"The Impact of Music Genres on Study Efficiency: Analyzing Distraction through Eye and Facial Movement Tracking through Biometrics.\" This notebook incorporates data collection, analysis, modeling, and visualization techniques required for your study."
      ],
      "metadata": {
        "id": "j7J51orU5686"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Setup\n",
        "#### Importing necessary libraries"
      ],
      "metadata": {
        "id": "ocTtQBWe62Pt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvRMJX1x5cLD"
      },
      "outputs": [],
      "source": [
        "# Notebook Title: The Impact of Music Genres on Study Efficiency: Analyzing Distraction through Biometrics\n",
        "\n",
        "# Step 1: Setup\n",
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import cv2\n",
        "import librosa\n",
        "from scipy.signal import find_peaks\n",
        "import time\n",
        "from statsmodels.tsa.ar_model import AutoReg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Application Development\n",
        "#### Loading Music"
      ],
      "metadata": {
        "id": "UrOhl-Gx69Nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Application Development\n",
        "# Loading Music\n",
        "music_genres = ['Classical', 'Jazz', 'Pop', 'Electronic']\n",
        "selected_genre = input(f\"Select a music genre from {music_genres}: \")"
      ],
      "metadata": {
        "id": "p-1zbotf6_xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Step 3: Biometric Data Collection and Preprocessing"
      ],
      "metadata": {
        "id": "5FYy6PzF7CnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enhancements for Step 3: Biometric Data Collection and Preprocessing**\n",
        "\n",
        "    **Dataset:**\n",
        "    We will use a pre-existing dataset (e.g., FER2013, a dataset for facial expression recognition with labels like 'happy', 'sad', 'angry', etc.).\n",
        "    Preprocessing: Load the dataset, preprocess images (resize, normalize), and split them into training and testing sets.\n",
        "    Model Training: We'll modify the code to train the CNN model using the loaded facial expression dataset."
      ],
      "metadata": {
        "id": "HBO-l52W7JjR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "    load_facial_expression_dataset: This function reads images from a dataset directory, preprocesses them by resizing to 48x48 (standard size for FER2013), normalizes them, and converts labels to categorical format.\n",
        "    \n",
        "    ImageDataGenerator: This allows you to augment the training data by applying random transformations like rotations, zooms, and flips, which can help the model generalize better."
      ],
      "metadata": {
        "id": "UAg3piB87tXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Biometric Data Collection and Preprocessing\n",
        "# Instead of capturing video, we'll load a facial expression dataset (e.g., FER2013).\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "import os\n",
        "import cv2"
      ],
      "metadata": {
        "id": "4LadzWZM7YWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to load and preprocess the facial expression dataset\n",
        "def load_facial_expression_dataset(dataset_path):\n",
        "    image_size = (48, 48)  # Image size for FER2013\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    # Assuming FER2013 dataset structure (LATER replace with your dataset path and structure)\n",
        "    for emotion in os.listdir(dataset_path):\n",
        "        emotion_folder = os.path.join(dataset_path, emotion)\n",
        "        if os.path.isdir(emotion_folder):\n",
        "            for image_file in os.listdir(emotion_folder):\n",
        "                image_path = os.path.join(emotion_folder, image_file)\n",
        "                # Read image in grayscale\n",
        "                img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "                # Resize the image\n",
        "                img = cv2.resize(img, image_size)\n",
        "                images.append(img)\n",
        "                # The emotion label is the folder name\n",
        "                labels.append(emotion)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    images = np.array(images).reshape(-1, 48, 48, 1)  # Add channel dimension for grayscale\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Normalize the images\n",
        "    images = images / 255.0\n",
        "\n",
        "    # Convert labels to numerical categories\n",
        "    unique_labels = sorted(list(set(labels)))\n",
        "    label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    labels = np.array([label_to_index[label] for label in labels])\n",
        "\n",
        "    return images, labels, label_to_index\n",
        "\n",
        "# Path to your facial expression dataset\n",
        "dataset_path = '/facial/expression/dataset'  # e.g., FER2013 directory\n",
        "images, labels, label_to_index = load_facial_expression_dataset(dataset_path)\n",
        "\n",
        "# Convert labels to categorical (for classification)\n",
        "labels_categorical = to_categorical(labels)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "\n",
        "# Data Augmentation (optional for improving training)\n",
        "datagen = ImageDataGenerator(rotation_range=10, zoom_range=0.1, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "datagen.fit(X_train)"
      ],
      "metadata": {
        "id": "jY99lrUU7cAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Eye Movement Analysis (Focus Assessment)\n",
        "#### Defining a function to quantify focus or distraction from eye movement data"
      ],
      "metadata": {
        "id": "2CJrhuYw78v-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Eye Movement Analysis (Focus Assessment)\n",
        "# Defining a function to quantify focus or distraction from eye movement data\n",
        "def analyze_eye_movement(data, window_size):\n",
        "    focus_variability = []\n",
        "    for i in range(0, len(data), window_size):\n",
        "        window_data = data[i:i + window_size]\n",
        "        variability = np.std(window_data)\n",
        "        focus_variability.append(variability)\n",
        "    return focus_variability\n",
        "\n",
        "# Applying Eye Movement Analysis\n",
        "focus_variability = analyze_eye_movement(eye_movement_data, 50)\n",
        "plt.plot(focus_variability)\n",
        "plt.title('Eye Movement Variability (Focus Assessment)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TE9XCeyU77M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Facial Expression Analysis (Emotional Response Assessment)\n",
        "### Defining a function to analyze facial expression changes over time"
      ],
      "metadata": {
        "id": "myf6cbUP8May"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Facial Expression Analysis (Emotional Response Assessment)\n",
        "# Defining a function to analyze facial expression changes over time\n",
        "def analyze_facial_expressions(data, window_size):\n",
        "    expression_variability = []\n",
        "    for i in range(0, len(data), window_size):\n",
        "        window_data = data[i:i + window_size]\n",
        "        variability = np.std(window_data)\n",
        "        expression_variability.append(variability)\n",
        "    return expression_variability\n",
        "\n",
        "# Applying Facial Expression Analysis\n",
        "facial_expression_variability = analyze_facial_expressions(facial_expression_data, 50)\n",
        "plt.plot(facial_expression_variability)\n",
        "plt.title('Facial Expression Variability (Emotional Response)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AxYODsTm8KuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Statistical and Predictive Modeling\n",
        "### Statistical and Predictive Modeling using CNN for facial expression recognition"
      ],
      "metadata": {
        "id": "CRLRIj9C8VoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of the CNN:**\n",
        "\n",
        "    Model Architecture:\n",
        "        Three convolutional layers (Conv2D) with ReLU activation and max pooling layers (MaxPooling2D).\n",
        "        A fully connected layer followed by a dropout layer (Dropout) to prevent overfitting.\n",
        "        The output layer uses softmax activation, suitable for multi-class classification (emotion categories).\n",
        "        \n",
        "    Training: The model is trained on the augmented dataset using Adam optimizer and categorical crossentropy loss. Training and validation accuracy are plotted."
      ],
      "metadata": {
        "id": "tKjx44sY9PrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Statistical and Predictive Modeling\n",
        "# Statistical and Predictive Modeling using CNN for facial expression recognition\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
      ],
      "metadata": {
        "id": "OKKO5RxY8PTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model for facial expression recognition\n",
        "def build_cnn_model(input_shape, num_classes):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "Wu7S6CXQ8zbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and compile the model\n",
        "input_shape = (48, 48, 1)  # FER2013 image dimensions (48x48 grayscale)\n",
        "num_classes = len(label_to_index)  # Number of emotion categories\n",
        "model = build_cnn_model(input_shape, num_classes)\n",
        "\n"
      ],
      "metadata": {
        "id": "xbmr9b929HQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "batch_size = 64\n",
        "epochs = 30\n",
        "\n",
        "history = model.fit(datagen.flow(X_train, y_train, batch_size=batch_size), validation_data=(X_test, y_test), epochs=epochs)\n",
        "\n"
      ],
      "metadata": {
        "id": "cxoSLRfb9IkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "nzd7WmZ79J36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model on test set and get predictions\n"
      ],
      "metadata": {
        "id": "kMj90i_D9aXa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Breakdown of Features:**\n",
        "\n",
        "    Classification Report: Provides precision, recall, F1-score, and support for each class (emotion label).\n",
        "        Precision: How many selected items are relevant.\n",
        "        Recall: How many relevant items are selected.\n",
        "        F1-score: Harmonic mean of precision and recall.\n",
        "\n",
        "\n",
        "    Confusion Matrix: Helps visualize the performance of the classification model by showing where the model gets confused between classes. It's presented as a heatmap for better readability.\n",
        "\n",
        "\n",
        "    Accuracy Score: A common metric in classification tasks that shows the overall percentage of correctly classified samples.\n",
        "    \n",
        "\n",
        "    Scatter Plot: Shows how well the predicted labels match the actual labels. While scatter plots are often used in regression, they can still give a useful visual comparison in classification tasks, especially when you want to see where the model's predictions are concentrated."
      ],
      "metadata": {
        "id": "kBqF8wbT95E7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup of Evaluate the model on test set and get predictions\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "N05-dNGK9fiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PAK-mo1X94DY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on test set and get predictions\n",
        "y_pred_prob = model.predict(X_test)  # Probabilities from the model\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)  # Convert probabilities to class labels\n",
        "y_test_labels = np.argmax(y_test, axis=1)  # Convert one-hot encoded y_test to class labels\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_labels, y_pred, target_names=label_to_index.keys()))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test_labels, y_pred)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_to_index.keys(), yticklabels=label_to_index.keys())\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Calculate R^2 Score Equivalent for Classification (Accuracy)\n",
        "test_accuracy = accuracy_score(y_test_labels, y_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
        "\n",
        "# Scatter Plot: Actual vs Predicted Classes\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(y_test_labels, y_pred, alpha=0.5)\n",
        "plt.title('Actual vs Predicted Emotion Labels')\n",
        "plt.xlabel('Actual Emotion Label')\n",
        "plt.ylabel('Predicted Emotion Label')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g10xlLDv9mzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Additional Enhancements:**\n",
        "\n",
        "**Precision, Recall, and F1-scores** give deeper insights into your model's performance, especially in cases of imbalanced datasets where accuracy alone might be misleading.\n",
        "**Confusion matrix heatmap** visually pinpoints which emotions the model often confuses, helping you understand the model's strengths and weaknesses.\n",
        "\n",
        "This enhanced evaluation provides more detailed insights into how well your CNN is performing in terms of classification and helps visualize the comparison between actual and predicted classes. You can further adjust these metrics and visualizations based on your project's specific needs."
      ],
      "metadata": {
        "id": "ysw0_PxC-BRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of the Code**:\n",
        "\n",
        "    Precision, Recall, and F1-scores:\n",
        "        Precision: Measures how many of the predicted labels were actually correct for each class.\n",
        "        Recall: Measures how many of the actual labels were correctly predicted for each class.\n",
        "        F1-score: Harmonic mean of precision and recall, giving an overall score for each class.\n",
        "        The average='weighted' option calculates these metrics across all classes, weighted by the number of samples in each class (useful if the dataset is imbalanced).\n",
        "\n",
        "    Confusion Matrix Heatmap:\n",
        "        A visual representation of the confusion matrix using a heatmap, where the diagonal values represent the correct predictions, and off-diagonal values show where the model made incorrect predictions.\n",
        "        The x-axis represents the predicted class labels, while the y-axis represents the true class labels.\n",
        "\n",
        "    Classification Report:\n",
        "        A detailed report that includes precision, recall, and F1-score for each class, as well as support (number of samples) for each class.\n",
        "\n",
        "    Accuracy Score:\n",
        "        The overall accuracy of the model is displayed, showing the percentage of correct predictions across the entire test set.\n",
        "\n",
        "    Scatter Plot:\n",
        "        This plot compares the actual vs predicted class labels, providing a visual sense of how well the model is performing. Ideally, the points should form a diagonal line if the predictions match the actual labels perfectly."
      ],
      "metadata": {
        "id": "4szbdXeA_EKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Additional Enhancements\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "hYvCmYQ59LSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The model has been trained and now you are using X_test to evaluate it.\n",
        "\n",
        "# Predicting the probabilities for the test set\n",
        "y_pred_prob = model.predict(X_test)  # Probabilities from the CNN model\n",
        "\n",
        "# Converting probabilities to predicted class labels\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)  # Convert to predicted class labels\n",
        "y_test_labels = np.argmax(y_test, axis=1)  # Convert one-hot encoded test labels to original class labels\n",
        "\n",
        "# 1. Precision, Recall, F1-scores\n",
        "\n",
        "# Calculate precision, recall, and F1-score for each class\n",
        "precision = precision_score(y_test_labels, y_pred, average=None)  # Precision per class\n",
        "recall = recall_score(y_test_labels, y_pred, average=None)  # Recall per class\n",
        "f1 = f1_score(y_test_labels, y_pred, average=None)  # F1-score per class\n",
        "\n"
      ],
      "metadata": {
        "id": "jrMuyxTf-m1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing Precision, Recall, and F1-score for each class\n",
        "for i, label in enumerate(label_to_index.keys()):  # Assuming label_to_index is the dictionary for emotion labels\n",
        "    print(f\"Class: {label}\")\n",
        "    print(f\"  Precision: {precision[i]:.2f}\")\n",
        "    print(f\"  Recall:    {recall[i]:.2f}\")\n",
        "    print(f\"  F1-Score:  {f1[i]:.2f}\")\n",
        "    print()\n",
        "\n",
        "# Overall precision, recall, F1-score (averaged over all classes)\n",
        "overall_precision = precision_score(y_test_labels, y_pred, average='weighted')\n",
        "overall_recall = recall_score(y_test_labels, y_pred, average='weighted')\n",
        "overall_f1 = f1_score(y_test_labels, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Overall Precision: {overall_precision:.2f}\")\n",
        "print(f\"Overall Recall:    {overall_recall:.2f}\")\n",
        "print(f\"Overall F1-Score:  {overall_f1:.2f}\")\n",
        "print()\n",
        "\n"
      ],
      "metadata": {
        "id": "qRB2Xw48-pZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Confusion Matrix Heatmap\n",
        "\n",
        "# Generate the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test_labels, y_pred)\n",
        "\n",
        "# Plot the confusion matrix using Seaborn heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_to_index.keys(), yticklabels=label_to_index.keys())\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "B8vtzEe5-qzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Classification Report\n",
        "\n",
        "# Generate a detailed classification report\n",
        "class_report = classification_report(y_test_labels, y_pred, target_names=label_to_index.keys())\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n",
        "\n"
      ],
      "metadata": {
        "id": "9fnrjIps-s7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Accuracy Score\n",
        "\n",
        "# Calculate accuracy\n",
        "test_accuracy = accuracy_score(y_test_labels, y_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "x7Tp1Mj3-uRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Scatter Plot: Actual vs Predicted\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(y_test_labels, y_pred, alpha=0.5, edgecolor='k')\n",
        "plt.title('Actual vs Predicted Emotion Labels')\n",
        "plt.xlabel('Actual Emotion Label')\n",
        "plt.ylabel('Predicted Emotion Label')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6h_QbzsV-vW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More Evaluations for Multiclass level\n",
        "\n",
        "Evaluating your CNN model using Cohen's Kappa and Matthews Correlation Coefficient (MCC) in addition to the previous metrics. These metrics are particularly useful for evaluating multi-class models and imbalanced datasets."
      ],
      "metadata": {
        "id": "h4YzMq3S_Q3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score, matthews_corrcoef\n",
        "\n",
        "# The model has been trained and y_pred and y_test_labels are available\n",
        "\n",
        "# Predicting probabilities for the test set\n",
        "y_pred_prob = model.predict(X_test)  # Probabilities from the CNN model\n",
        "\n",
        "# Converting probabilities to predicted class labels\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)  # Convert to predicted class labels\n",
        "y_test_labels = np.argmax(y_test, axis=1)  # Convert one-hot encoded test labels to original class labels\n",
        "\n"
      ],
      "metadata": {
        "id": "8BDNvAyz_WsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Cohen's Kappa\n",
        "\n",
        "# Calculate Cohen's Kappa Score\n",
        "kappa_score = cohen_kappa_score(y_test_labels, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.2f}\")\n",
        "\n",
        "# 7. Matthews Correlation Coefficient (MCC)\n",
        "\n",
        "# Calculate MCC\n",
        "mcc = matthews_corrcoef(y_test_labels, y_pred)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8TNB-NLZ_dZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "----"
      ],
      "metadata": {
        "id": "FgDL4j7V_jpN"
      }
    }
  ]
}