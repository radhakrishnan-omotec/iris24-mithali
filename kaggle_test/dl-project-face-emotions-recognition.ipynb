{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1351797,"sourceType":"datasetVersion","datasetId":786787},{"sourceId":13651,"sourceType":"modelInstanceVersion","modelInstanceId":11295,"modelId":15787}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem Statement\n\n**Title**: Developing an Emotion Recognition System for Real-Time Video Streams\n\n\n\n**Description**:\n\nThis project aims to create a **deep learning model** capable of recognizing human emotions in real-time video streams. By processing live video data, the system will **detect and classify** facial expressions, contributing to applications in affective computing, human-computer interaction, and sentiment analysis. The project involves **data processing, model training, real-time inference** implementation, and performance evaluation, ultimately leading to a deployable emotion recognition system.\n\n\n**Tasks:**\n\n* **Data Loading**: Prepare and load the dataset for analysis.\n* **Data OverSampling**: Address class imbalance by increasing samples in minority classes.\n* **PCA & Outliers**: Reduce dimensionality with PCA and remove outliers for cleaner data.\n* **DL Model Building**: Design the architecture for the deep learning model.\n* **Training DL Model**: Train the model using the prepared data.\n* **Evaluation & Metrics**: Assess model performance using various metrics.\n* **Real Video Test**: Apply the model to real-world video data for evaluation.\n* **LIVE Feed Video Test**: Test the model's performance on live video feeds for real-time analysis.","metadata":{}},{"cell_type":"markdown","source":"# About Dataset\n\nThe dataset chosen for this project is **FER2013**, which is a widely-used benchmark dataset for facial expression recognition. FER2013 consists of **48x48-pixel grayscale images** of human faces categorized into seven different emotions: anger, disgust, fear, happiness, sadness, surprise, and neutral. \n\nThe dataset is split into two sets: training,and test sets, containing 28,709, and 7,178 images, respectively.\n","metadata":{}},{"cell_type":"markdown","source":"# Data Loading\n\nIn this section, I begin by reading the train and test images from their respective folders. Using appropriate functions, such as cv2.imread(), Once the images are loaded, I convert them into numpy arrays, along with their respective labels. \n\nEach image is associated with a label indicating the emotion it represents.","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport warnings\n\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfrom sklearn.decomposition import PCA\nfrom scipy.spatial import distance\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom IPython.display import display, HTML, Video\n\nfrom PIL import Image\nimport urllib.request\n\n\n# Suppress FutureWarning\nwarnings.simplefilter(action='ignore', category=FutureWarning)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:15:28.78137Z","iopub.execute_input":"2024-03-04T11:15:28.781653Z","iopub.status.idle":"2024-03-04T11:15:34.633844Z","shell.execute_reply.started":"2024-03-04T11:15:28.781626Z","shell.execute_reply":"2024-03-04T11:15:34.632896Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_folder = '/kaggle/input/fer2013/train'\ntest_folder = '/kaggle/input/fer2013/test'","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:15:34.635394Z","iopub.execute_input":"2024-03-04T11:15:34.635798Z","iopub.status.idle":"2024-03-04T11:15:34.639905Z","shell.execute_reply.started":"2024-03-04T11:15:34.635773Z","shell.execute_reply":"2024-03-04T11:15:34.639036Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to load images from a folder\ndef load_images_from_folder(folder):\n    images = []\n    labels = []\n    for emotion_folder in os.listdir(folder):\n        label = emotion_folder\n        for filename in os.listdir(os.path.join(folder, emotion_folder)):\n            img = cv2.imread(os.path.join(folder, emotion_folder, filename), cv2.IMREAD_GRAYSCALE)\n            if img is not None:\n                images.append(img)\n                labels.append(label)\n    return images, labels","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:15:34.641334Z","iopub.execute_input":"2024-03-04T11:15:34.641589Z","iopub.status.idle":"2024-03-04T11:15:34.651729Z","shell.execute_reply.started":"2024-03-04T11:15:34.641567Z","shell.execute_reply":"2024-03-04T11:15:34.650905Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load images and labels from train and test folders\ntrain_images, train_labels = load_images_from_folder(train_folder)\ntest_images, test_labels = load_images_from_folder(test_folder)\n\n# Convert lists to numpy arrays\ntrain_images = np.array(train_images)\ntrain_labels = np.array(train_labels)\ntest_images = np.array(test_images)\ntest_labels = np.array(test_labels)\n\n# Verify the shape of the datasets\nprint(\"Train images shape:\", train_images.shape)\nprint(\"Train labels shape:\", train_labels.shape)\nprint(\"Test images shape:\", test_images.shape)\nprint(\"Test labels shape:\", test_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:15:34.653782Z","iopub.execute_input":"2024-03-04T11:15:34.654041Z","iopub.status.idle":"2024-03-04T11:20:22.483827Z","shell.execute_reply.started":"2024-03-04T11:15:34.654019Z","shell.execute_reply":"2024-03-04T11:20:22.482884Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Afterward, to get a better understanding of the dataset, I sample some images from each class and display them as a grid. This visualization aids in inspecting the dataset's distribution and the nature of images present.","metadata":{}},{"cell_type":"code","source":"# Function to select sample images from each class\ndef select_sample_images(images, labels, num_samples=5):\n    sample_images = []\n    class_names = np.unique(labels)\n    for class_name in class_names:\n        class_indices = np.where(labels == class_name)[0][:num_samples]\n        sample_images.extend(images[class_indices])\n    return sample_images, class_names","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:20:22.48488Z","iopub.execute_input":"2024-03-04T11:20:22.485157Z","iopub.status.idle":"2024-03-04T11:20:22.490586Z","shell.execute_reply.started":"2024-03-04T11:20:22.485133Z","shell.execute_reply":"2024-03-04T11:20:22.489657Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_samples = 10\n\n# Select sample images and class names\nsample_images, class_names = select_sample_images(train_images, train_labels, num_samples)\n\n# Plot sample images in a grid\nnum_classes = len(class_names)\nfig, axes = plt.subplots(num_classes, num_samples, figsize=(15, 15))\nfor i, class_name in enumerate(class_names):\n    class_indices = np.where(train_labels == class_name)[0][:num_samples]\n    for j, idx in enumerate(class_indices):\n        axes[i, j].imshow(train_images[idx], cmap='gray')\n        axes[i, j].axis('off')\n        axes[i, j].set_title(class_name, fontsize=10)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:20:22.491668Z","iopub.execute_input":"2024-03-04T11:20:22.491926Z","iopub.status.idle":"2024-03-04T11:20:27.49015Z","shell.execute_reply.started":"2024-03-04T11:20:22.491904Z","shell.execute_reply":"2024-03-04T11:20:27.489157Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data OverSampling","metadata":{}},{"cell_type":"code","source":"# Calculate the count of images for each class in the training set\ntrain_class_counts = {class_name: np.sum(train_labels == class_name) for class_name in np.unique(train_labels)}\n\n# Create a bar chart using Plotly Express for training set only\nfig = px.bar(x=list(train_class_counts.keys()), y=list(train_class_counts.values()), color=list(train_class_counts.keys()))\n\n# Customize the layout of the chart\nfig.update_layout(\n    title=\"Total samples for each class (Training set only)\",\n    xaxis_title=\"Emotion\",\n    yaxis_title=\"Count\"\n)\nfig.update_traces(texttemplate='%{y}', textposition='inside')  # Display count labels on the bars\n\n# Show the plot\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:20:27.491493Z","iopub.execute_input":"2024-03-04T11:20:27.491817Z","iopub.status.idle":"2024-03-04T11:20:29.721203Z","shell.execute_reply.started":"2024-03-04T11:20:27.491789Z","shell.execute_reply":"2024-03-04T11:20:29.720321Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"it seems like the data is **imbalanced**. Some classes have significantly more samples than others. For example, classes like **happiness** have **7215** samples, while **disgust** has only **436** samples.\n\nImbalanced data can affect the **performance** of machine learning models, as they may become **biased** towards the majority class. Therefore, it's important to consider techniques such as **oversampling**.\n\nOversampling is like adding more synthetic images to dataset.\n\nTo address this Imbalanced data issue, I perform oversampling on the minority classes \"**disgust**\" and \"**surprise**\" using the Synthetic Minority Over-sampling Technique **(SMOTE)**. \n\nSMOTE generates synthetic samples for the minority class by **interpolating** between existing samples. This helps to balance the class distribution and improves the model's ability to learn from the minority classes.\n\nOversampling is a technique used to balance class distribution in imbalanced datasets by increasing the number of samples in minority classes through synthetic data generation or replication.\n","metadata":{}},{"cell_type":"markdown","source":"In a few lines, the process involves:\n\n* Identifying the indices of the minority classes (\"disgust\" and \"surprise\").\n* Resampling these classes using SMOTE to increase their sample count.\n* Concatenating the original data with the resampled data to create a balanced dataset.\n* Selecting a specific number of samples from the resampled data for certain classes to control the balance and size of the dataset.","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\n# Identify the indices of \"Disgust\" and \"Surprise\" classes\ndisgust_indices = np.where(train_labels == \"disgust\")[0]\nsurprise_indices = np.where(train_labels == \"surprise\")[0]\n\n# Combine the indices of both classes\nindices_to_oversample = np.concatenate([disgust_indices, surprise_indices])\n\n# Extract images and labels for the selected indices\nX_to_oversample = train_images[indices_to_oversample]\ny_to_oversample = train_labels[indices_to_oversample]\n\n# Reshape the images to 2D (if needed)\nX_to_oversample_2d = X_to_oversample.reshape(X_to_oversample.shape[0], -1)\n\n# Define the oversampling strategy\noversample = SMOTE()\n\n# Apply SMOTE to the selected classes\nX_resampled, y_resampled = oversample.fit_resample(X_to_oversample_2d, y_to_oversample)\n\n# Reshape the oversampled data back to its original shape\nX_resampled = X_resampled.reshape(-1, *train_images.shape[1:])\n\n# Check the new class distribution\nunique, counts = np.unique(y_resampled, return_counts=True)\nprint(dict(zip(unique, counts)))","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:20:29.722482Z","iopub.execute_input":"2024-03-04T11:20:29.722783Z","iopub.status.idle":"2024-03-04T11:20:30.408623Z","shell.execute_reply.started":"2024-03-04T11:20:29.722758Z","shell.execute_reply":"2024-03-04T11:20:30.407221Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Plotting **original** and **oversampled disgust** images allows visualizing the effect of oversampling on the class distribution. ","metadata":{}},{"cell_type":"code","source":"# Filter oversampled data for \"disgust\" class\ndisgust_resampled_indices = np.where(y_resampled == \"disgust\")[0]\n\n# Choose random samples from the original \"disgust\" data\nnum_samples = 5\nrandom_original_indices = np.random.choice(disgust_indices, num_samples, replace=False)\n\n# Choose random samples from the oversampled data for \"disgust\" class\nnum_oversampled = min(num_samples, len(disgust_resampled_indices))\nrandom_over_sample_indices = np.random.choice(disgust_resampled_indices, num_oversampled, replace=False)\n\n# Plot original images\nplt.figure(figsize=(15, 5))\nfor i, idx in enumerate(random_original_indices):\n    plt.subplot(2, num_samples, i + 1)\n    plt.imshow(train_images[idx], cmap='gray')  # Original image\n    plt.title(\"Original : Disgust\", fontsize=10)\n    plt.axis('off')\n\n# Plot oversampled images\nfor i, idx in enumerate(random_over_sample_indices):\n    plt.subplot(2, num_samples, num_samples + i + 1)\n    plt.imshow(X_resampled[idx], cmap='gray')  # Oversampled image\n    plt.title(\"Oversampled : Disgust\", fontsize=10)\n    plt.axis('off')\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:20:30.41098Z","iopub.execute_input":"2024-03-04T11:20:30.41242Z","iopub.status.idle":"2024-03-04T11:20:31.041207Z","shell.execute_reply.started":"2024-03-04T11:20:30.412363Z","shell.execute_reply":"2024-03-04T11:20:31.040445Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Plotting **original** and **oversampled** **surprise** images allows visualizing the effect of oversampling on the class distribution. ","metadata":{}},{"cell_type":"code","source":"# Filter oversampled data for \"disgust\" class\nsurprise_resampled_indices = np.where(y_resampled == \"surprise\")[0]\n\n# Choose random samples from the original \"disgust\" data\nnum_samples = 5\nrandom_original_indices = np.random.choice(surprise_indices, num_samples, replace=False)\n\n# Choose random samples from the oversampled data for \"disgust\" class\nnum_oversampled = min(num_samples, len(surprise_resampled_indices))\nrandom_over_sample_indices = np.random.choice(surprise_resampled_indices, num_oversampled, replace=False)\n\n# Plot original images\nplt.figure(figsize=(15, 5))\nfor i, idx in enumerate(random_original_indices):\n    plt.subplot(2, num_samples, i + 1)\n    plt.imshow(train_images[idx], cmap='gray')  # Original image\n    plt.title(\"Original : Surprise\", fontsize=10)\n    plt.axis('off')\n\n# Plot oversampled images\nfor i, idx in enumerate(random_over_sample_indices):\n    plt.subplot(2, num_samples, num_samples + i + 1)\n    plt.imshow(X_resampled[idx], cmap='gray')  # Oversampled image\n    plt.title(\"Oversampled : Surprise\", fontsize=10)\n    plt.axis('off')\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:20:31.044437Z","iopub.execute_input":"2024-03-04T11:20:31.044721Z","iopub.status.idle":"2024-03-04T11:20:31.712928Z","shell.execute_reply.started":"2024-03-04T11:20:31.044697Z","shell.execute_reply":"2024-03-04T11:20:31.712014Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Selecting a specific **number of samples** from the resampled data for certain classes to control the balance and size of the dataset.","metadata":{}},{"cell_type":"code","source":"# Define the maximum number of samples for each class\nmax_surprise_samples = 500\nmax_disgust_samples = 2000\n\n# Identify the indices of the \"surprise\" and \"disgust\" classes in the resampled data\nsurprise_indices = np.where(y_resampled == \"surprise\")[0]\ndisgust_indices = np.where(y_resampled == \"disgust\")[0]\n\n# Select the first 500 samples for \"surprise\" and the first 1500 samples for \"disgust\"\nselected_surprise_indices = surprise_indices[:max_surprise_samples]\nselected_disgust_indices = disgust_indices[:max_disgust_samples]\n\n# Concatenate the original train images with the selected oversampled images\nfinal_train_images = np.concatenate([train_images, X_resampled[selected_surprise_indices], X_resampled[selected_disgust_indices]], axis=0)\n\n# Create labels for the selected oversampled images\nselected_surprise_labels = np.full(len(selected_surprise_indices), \"surprise\")\nselected_disgust_labels = np.full(len(selected_disgust_indices), \"disgust\")\n\n# Concatenate the original train labels with the selected oversampled labels\nfinal_train_labels = np.concatenate([train_labels, selected_surprise_labels, selected_disgust_labels], axis=0)\n\nfinal_train_images.shape, final_train_labels.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:20:31.714147Z","iopub.execute_input":"2024-03-04T11:20:31.714513Z","iopub.status.idle":"2024-03-04T11:20:31.754293Z","shell.execute_reply.started":"2024-03-04T11:20:31.714475Z","shell.execute_reply":"2024-03-04T11:20:31.75324Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the count of images for each class in the training set\ntrain_class_counts = {class_name: np.sum(final_train_labels == class_name) for class_name in np.unique(final_train_labels)}\n\n# Create a bar chart using Plotly Express for training set only\nfig = px.bar(x=list(train_class_counts.keys()), y=list(train_class_counts.values()), color=list(train_class_counts.keys()))\n\n# Customize the layout of the chart\nfig.update_layout(\n    title=\"Total samples for each class after oversampling (Training set only)\",\n    xaxis_title=\"Emotion\",\n    yaxis_title=\"Count\"\n)\nfig.update_traces(texttemplate='%{y}', textposition='inside')  # Display count labels on the bars\n\n# Show the plot\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:20:31.755679Z","iopub.execute_input":"2024-03-04T11:20:31.7563Z","iopub.status.idle":"2024-03-04T11:20:31.849985Z","shell.execute_reply.started":"2024-03-04T11:20:31.75624Z","shell.execute_reply":"2024-03-04T11:20:31.849229Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_train_images.shape, final_train_labels.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:20:31.850896Z","iopub.execute_input":"2024-03-04T11:20:31.851148Z","iopub.status.idle":"2024-03-04T11:20:31.856841Z","shell.execute_reply.started":"2024-03-04T11:20:31.851125Z","shell.execute_reply":"2024-03-04T11:20:31.855938Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, The data looks **balanced**.\n\n\n\n\n----------------------------------\n","metadata":{}},{"cell_type":"markdown","source":"# PCA & Outliers","metadata":{}},{"cell_type":"code","source":"# Reshape the image data to a 2D array\nnum_samples, height, width = final_train_images.shape\n\nX_train_flattened = final_train_images.reshape(num_samples, height * width)\n\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train_flattened)\nX_train_pca.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:20:31.857848Z","iopub.execute_input":"2024-03-04T11:20:31.858135Z","iopub.status.idle":"2024-03-04T11:20:35.031234Z","shell.execute_reply.started":"2024-03-04T11:20:31.858108Z","shell.execute_reply":"2024-03-04T11:20:35.030271Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize the PCA-transformed data\n\nhover_text = [f\"Index: {index}\" for index in range(len(X_train_pca))]\n\nfig = px.scatter(x=X_train_pca[:, 0], y=X_train_pca[:, 1], color=final_train_labels, hover_name=hover_text, symbol=final_train_labels, title='PCA Visualization of Image Classes')\nfig.update_traces(marker=dict(size=15))\nfig.update_layout(xaxis_title='Principal Component 1', yaxis_title='Principal Component 2')\nfig.update_layout(coloraxis_showscale=False)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:20:35.032199Z","iopub.execute_input":"2024-03-04T11:20:35.032473Z","iopub.status.idle":"2024-03-04T11:20:35.292347Z","shell.execute_reply.started":"2024-03-04T11:20:35.03245Z","shell.execute_reply":"2024-03-04T11:20:35.291457Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I applied Principal Component Analysis (PCA) to visualize high-dimensional image data in a lower-dimensional space. \n\nPCA helped us identify outliers by comparing the original images with their reconstructed versions. By setting a threshold for the reconstruction error, images with significantly higher errors were flagged as outliers. \n","metadata":{}},{"cell_type":"code","source":"# Reconstruct images\nX_reconstructed = pca.inverse_transform(X_train_pca)\n\n# Calculate reconstruction error\nreconstruction_errors = np.sqrt(np.mean(np.square(X_train_flattened - X_reconstructed), axis=1))\n\n# Set threshold for outlier detection\nthreshold = 100  # Adjust as needed\n\n# Find outliers\noutlier_indices = np.where(reconstruction_errors > threshold)[0]\n\nprint(\"total outliers = \", outlier_indices.shape)\n\nnum_outliers_to_visualize = 6  # Choose the number of outliers to visualize\n\nplt.figure(figsize=(15, 5))\nfor i, idx in enumerate(outlier_indices[:num_outliers_to_visualize]):\n    plt.subplot(1, num_outliers_to_visualize, i + 1)\n    plt.imshow(final_train_images[idx], cmap='gray')\n    plt.title(f\"{final_train_labels[idx]} \\n Error, {reconstruction_errors[idx]:.2f}\")\n    plt.axis('off')\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:20:35.293543Z","iopub.execute_input":"2024-03-04T11:20:35.29389Z","iopub.status.idle":"2024-03-04T11:20:36.648067Z","shell.execute_reply.started":"2024-03-04T11:20:35.293865Z","shell.execute_reply":"2024-03-04T11:20:36.647144Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Perform PCA with 3 compoinents\n\npca = PCA(n_components=3)\nX_train_pca_3d = pca.fit_transform(X_train_flattened)\n\n# Calculate the centroid of the main cluster\ncentroid = np.mean(X_train_pca_3d, axis=0)\n\n# Calculate distances from the centroid\ndistances = distance.cdist([centroid], X_train_pca_3d, 'euclidean')[0]\n\n# Define a threshold for outlier detection\nthreshold = np.percentile(distances, 99.5)  # Adjust the percentile as needed\n\n# Identify outliers\noutlier_indices = np.where(distances > threshold)[0]\n\n# Define custom colors for outliers\noutlier_color = 'rgb(0, 0, 0)'  # Black\n\n# Define custom colors for each class\nclass_colors = {\n    'angry': 'rgb(255, 0, 0)',  # Red\n    'disgust': 'rgb(0, 255, 0)',  # Green\n    'fear': 'rgb(0, 0, 255)',  # Blue\n    'happy': 'rgb(255, 255, 0)',  # Yellow\n    'neutral': 'rgb(255, 165, 0)',  # Orange\n    'sad': 'rgb(128, 0, 128)',  # Purple\n    'surprise': 'rgb(0, 128, 128)'  # Teal\n}\n\n# Create traces for each class\ntraces = []\nfor class_label, color in class_colors.items():\n    indices = np.where(final_train_labels == class_label)[0]\n    trace = go.Scatter3d(\n        x=X_train_pca_3d[indices, 0],\n        y=X_train_pca_3d[indices, 1],\n        z=X_train_pca_3d[indices, 2],\n        mode='markers',\n        marker=dict(\n            size=5,\n            color=color,\n            opacity=0.8\n        ),\n        name=class_label\n    )\n    traces.append(trace)\n\n# Create trace for outliers with hovertext including class names\noutlier_hovertext = [f\"Class: {_}\" for _ in final_train_labels[outlier_indices]]\noutlier_trace = go.Scatter3d(\n    x=X_train_pca_3d[outlier_indices, 0],\n    y=X_train_pca_3d[outlier_indices, 1],\n    z=X_train_pca_3d[outlier_indices, 2],\n    mode='markers',\n    marker=dict(\n        size=5,\n        color=outlier_color,\n        opacity=0.8\n    ),\n    name='Outliers',\n    hovertext=outlier_hovertext\n)\n\ntraces.append(outlier_trace)\n\n# Create layout with legend\nlayout = go.Layout(\n    title='PCA Visualization of Image Classes with Outliers (3D)',\n    scene=dict(\n        xaxis_title='Principal Component 1',\n        yaxis_title='Principal Component 2',\n        zaxis_title='Principal Component 3'\n    ),\n    legend=dict(\n        title='Classes',\n        yanchor='top',\n        y=0.99,\n        xanchor='left',\n        x=0.01\n    )\n)\n\n# Create figure with traces and layout\nfig = go.Figure(data=traces, layout=layout)\n\n# Show the plot\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:20:36.649834Z","iopub.execute_input":"2024-03-04T11:20:36.650432Z","iopub.status.idle":"2024-03-04T11:20:39.815807Z","shell.execute_reply.started":"2024-03-04T11:20:36.650391Z","shell.execute_reply":"2024-03-04T11:20:39.814913Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\nnum_classes = len(class_names)\nnum_samples = 5 \n\nfig, axes = plt.subplots(num_classes, num_samples, figsize=(15, 15))\n\nfor i, class_name in enumerate(class_names):\n    class_indices = np.where(final_train_labels == class_name)[0]\n    outlier_indices_for_class = np.intersect1d(class_indices, outlier_indices)  # Get outliers for current class\n    num_outliers = len(outlier_indices_for_class)\n    \n    print(\"Outliers for {} = {}\".format(class_name, num_outliers))\n    \n    # Select random outliers if there are more than num_samples\n    if num_outliers > num_samples:\n        outlier_indices_for_class = np.random.choice(outlier_indices_for_class, num_samples, replace=False)\n    \n    for j, outlier_idx in enumerate(outlier_indices_for_class):\n        ax = axes[i, j]\n        ax.imshow(final_train_images[outlier_idx], cmap='gray')\n        ax.set_title(f'Outlier: {class_name}')\n        ax.axis('off')\n\n# Remove empty subplots\nfor i in range(num_classes):\n    for j in range(num_samples, len(outlier_indices_for_class)):\n        axes[i, j].remove()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:20:39.816862Z","iopub.execute_input":"2024-03-04T11:20:39.817116Z","iopub.status.idle":"2024-03-04T11:20:42.539598Z","shell.execute_reply.started":"2024-03-04T11:20:39.817095Z","shell.execute_reply":"2024-03-04T11:20:42.538475Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove outliers from final_train_images and final_train_labels\nfinal_train_images_cleaned = np.delete(final_train_images, outlier_indices, axis=0)\nfinal_train_labels_cleaned = np.delete(final_train_labels, outlier_indices, axis=0)\n\nfinal_train_images.shape, final_train_images_cleaned.shape,","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:20:42.540961Z","iopub.execute_input":"2024-03-04T11:20:42.54127Z","iopub.status.idle":"2024-03-04T11:20:42.578276Z","shell.execute_reply.started":"2024-03-04T11:20:42.541225Z","shell.execute_reply":"2024-03-04T11:20:42.577355Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"----------------------------------\n","metadata":{}},{"cell_type":"markdown","source":"# DL Model Building","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:20:42.579462Z","iopub.execute_input":"2024-03-04T11:20:42.579744Z","iopub.status.idle":"2024-03-04T11:20:48.771442Z","shell.execute_reply.started":"2024-03-04T11:20:42.57972Z","shell.execute_reply":"2024-03-04T11:20:48.770555Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self, num_classes):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.fc1 = nn.Linear(128 * 6 * 6, 512)  # Adjust this depending on input size\n        self.fc2 = nn.Linear(512, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.pool(x)\n        x = self.relu(self.conv2(x))\n        x = self.pool(x)\n        x = self.relu(self.conv3(x))\n        x = self.pool(x)\n        x = x.view(-1, 128 * 6 * 6)\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:20:48.772622Z","iopub.execute_input":"2024-03-04T11:20:48.773146Z","iopub.status.idle":"2024-03-04T11:20:48.782706Z","shell.execute_reply.started":"2024-03-04T11:20:48.773118Z","shell.execute_reply":"2024-03-04T11:20:48.781888Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CNN2(nn.Module):\n    def __init__(self, num_classes):\n        super(CNN2, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.conv2 = nn.Conv2d(32, 32, kernel_size=5, padding=2)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.dropout1 = nn.Dropout2d(p=0.5)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.conv4 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n        self.dropout2 = nn.Dropout2d(p=0.5)\n        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.conv6 = nn.Conv2d(128, 128, kernel_size=5, padding=2)\n        self.dropout3 = nn.Dropout2d(p=0.5)\n        self.fc1 = nn.Linear(128 * 6 * 6, 256)\n        self.fc2 = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        x = self.dropout1(x)\n        x = F.relu(self.bn2(self.conv3(x)))\n        x = F.relu(self.conv4(x))\n        x = self.pool(x)\n        x = self.dropout2(x)\n        x = F.relu(self.bn3(self.conv5(x)))\n        x = F.relu(self.conv6(x))\n        x = self.pool(x)\n        x = self.dropout3(x)\n        x = x.view(-1, 128 * 6 * 6)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:20:48.78515Z","iopub.execute_input":"2024-03-04T11:20:48.785921Z","iopub.status.idle":"2024-03-04T11:20:48.826111Z","shell.execute_reply.started":"2024-03-04T11:20:48.785893Z","shell.execute_reply":"2024-03-04T11:20:48.825398Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I designed couple of models CNN and CNN2 which has a series of layers \n\n1. **Input Layer**: The input images are grayscale images with a single channel.\n\n2. **Convolutional Layers**: The model consists of several convolutional layers (`conv1`, `conv2`, `conv3`, `conv4`, `conv5`, `conv6`) followed by batch normalization layers (`bn1`, `bn2`, `bn3`) and rectified linear unit (ReLU) activation functions. These layers extract features from the input images by convolving learnable filters over the input image.\n\n3. **Pooling Layers**: Max-pooling layers (`pool`) are used to reduce the spatial dimensions of the feature maps, which helps in reducing the computational complexity and controlling overfitting.\n\n4. **Dropout Layers**: Dropout layers (`dropout1`, `dropout2`, `dropout3`) are added after some convolutional and pooling layers to randomly drop a fraction of the neuron units during training, which helps in regularizing the model and preventing overfitting.\n\n5. **Fully Connected Layers**: The output of the convolutional layers is flattened and fed into fully connected layers (`fc1`, `fc2`) for classification. These layers perform the final classification based on the features extracted by the convolutional layers.\n\n6. **Output Layer**: The output layer (`fc2`) produces the final output, which represents the predicted class probabilities. The number of units in this layer is equal to the number of classes in the classification task.\n\n7. **Activation Function**: The output of the last fully connected layer is passed through a softmax activation function to obtain the predicted probabilities for each class.\n\nIn summary, this CNN2 architecture consists of alternating convolutional, pooling, and dropout layers followed by fully connected layers for classification, with batch normalization and ReLU activation functions used throughout the network to improve learning and performance.","metadata":{}},{"cell_type":"code","source":"!pip install torchviz","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:20:48.827202Z","iopub.execute_input":"2024-03-04T11:20:48.827511Z","iopub.status.idle":"2024-03-04T11:21:05.800372Z","shell.execute_reply.started":"2024-03-04T11:20:48.827487Z","shell.execute_reply":"2024-03-04T11:21:05.799259Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchviz import make_dot","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:21:05.801751Z","iopub.execute_input":"2024-03-04T11:21:05.802066Z","iopub.status.idle":"2024-03-04T11:21:05.94333Z","shell.execute_reply.started":"2024-03-04T11:21:05.802038Z","shell.execute_reply":"2024-03-04T11:21:05.942611Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#training data\nX_train_tensor = torch.tensor(final_train_images_cleaned)\nX_train_tensor = X_train_tensor.float()\nX_train_tensor = torch.unsqueeze(X_train_tensor, 1)\n\nprint(X_train_tensor.shape, X_train_tensor.dtype)\n\n\n#testing data\nX_test_tensor = torch.tensor(test_images)\nX_test_tensor = X_test_tensor.float()\nX_test_tensor = torch.unsqueeze(X_test_tensor, 1)\n\nprint(X_test_tensor.shape, X_test_tensor.dtype)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:21:05.944335Z","iopub.execute_input":"2024-03-04T11:21:05.944601Z","iopub.status.idle":"2024-03-04T11:21:06.179549Z","shell.execute_reply.started":"2024-03-04T11:21:05.944578Z","shell.execute_reply":"2024-03-04T11:21:06.178616Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This above code prepares the input data for training and testing:\n\n* Converts the training and testing images into PyTorch tensors.\n* Converts the tensors to float type.\n* Adds an additional dimension to represent grayscale images.\n* Prints the shape and data type of the tensors.","metadata":{}},{"cell_type":"code","source":"#training data\nlabel_encoder = LabelEncoder()\ny_train_encoded = label_encoder.fit_transform(final_train_labels_cleaned)\ny_train_tensor = torch.tensor(y_train_encoded)\n\nclass_names = label_encoder.classes_\nprint(class_names)\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n#testing data\nlabel_encoder = LabelEncoder()\ny_test_encoded = label_encoder.fit_transform(test_labels)\ny_test_tensor = torch.tensor(y_test_encoded)\n\nclass_names = label_encoder.classes_\nprint(class_names)\n\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:21:06.180946Z","iopub.execute_input":"2024-03-04T11:21:06.181517Z","iopub.status.idle":"2024-03-04T11:21:06.197231Z","shell.execute_reply.started":"2024-03-04T11:21:06.181481Z","shell.execute_reply":"2024-03-04T11:21:06.196317Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This above code segment prepares the label data for deep learning model training:\n\n* It encodes class labels numerically.\n* Converts data into PyTorch tensors.\n* Creates datasets and data loaders for efficient training and testing.","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:21:06.198338Z","iopub.execute_input":"2024-03-04T11:21:06.198605Z","iopub.status.idle":"2024-03-04T11:21:06.255475Z","shell.execute_reply.started":"2024-03-04T11:21:06.198583Z","shell.execute_reply":"2024-03-04T11:21:06.25456Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"-----------------------","metadata":{}},{"cell_type":"markdown","source":"# Training DL Model","metadata":{}},{"cell_type":"markdown","source":"* Instantiates a CNN model for classification.\n* Specifies the number of output classes.\n* Moves the model to the specified device (e.g., GPU).\n* Defines CrossEntropyLoss as the loss function.\n* Chooses Adam optimizer with a learning rate of 0.001.","metadata":{}},{"cell_type":"code","source":"# Instantiate the model\nnum_classes = 7\nmodel = CNN2(num_classes)\nmodel.to(device)\n\n# Define your loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# # Save model architecture as image\n# dummy_input = torch.randn(1, 1, 48, 48)  # (batch_size, channels, height, width)\n# dot = make_dot(model(dummy_input), params=dict(model.named_parameters()))\n# dot.render(\"model_visualization\", format=\"png\")","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:32:44.565134Z","iopub.execute_input":"2024-03-04T11:32:44.56582Z","iopub.status.idle":"2024-03-04T11:32:44.598677Z","shell.execute_reply.started":"2024-03-04T11:32:44.565787Z","shell.execute_reply":"2024-03-04T11:32:44.597742Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Training works like this\n\n\n* It iterates over a specified number of epochs (num_epochs).\n* Within each epoch, it sets the model to training mode and initializes the running loss.\n* It iterates through batches of training data from the train_loader.\n* For each batch, it moves the inputs and labels to the specified device (e.g., GPU), zeroes the optimizer gradients, performs a forward pass through the model to obtain predictions, calculates the loss using the specified criterion (CrossEntropyLoss), performs backward propagation to compute gradients, and updates the model parameters via optimization (Adam).\n* It accumulates the running loss for each epoch.\n* After each epoch, it prints the average loss.\n* Finally, it saves the trained model's state dictionary to a file ('my_model.pth').","metadata":{}},{"cell_type":"code","source":"# Training loop\nnum_epochs = 700\nfinal_loss = []\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        \n        inputs, labels = inputs.to(device), labels.to(device)\n                \n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(inputs)\n\n        # Calculate the loss\n        loss = criterion(outputs, labels) \n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        \n    final_loss.append(running_loss)\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n        \n    if epoch % 50 == 0:        \n        model_name = \"fer_model_\"+ str(epoch)+ \".pth\"\n        torch.save(model.state_dict(), model_name)\n\n# Save the final trained model\ntorch.save(model.state_dict(), 'fer_model_final.pth')","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:32:55.879454Z","iopub.execute_input":"2024-03-04T11:32:55.879798Z","iopub.status.idle":"2024-03-04T13:24:17.134228Z","shell.execute_reply.started":"2024-03-04T11:32:55.879772Z","shell.execute_reply":"2024-03-04T13:24:17.13314Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting the final loss\nfig = go.Figure(data=go.Scatter(y=final_loss, mode='lines'))\nfig.update_layout(title='Training Loss over Epochs',\n                  xaxis_title='Epoch',\n                  yaxis_title='Loss')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T13:24:31.205489Z","iopub.execute_input":"2024-03-04T13:24:31.206212Z","iopub.status.idle":"2024-03-04T13:24:31.235548Z","shell.execute_reply.started":"2024-03-04T13:24:31.206174Z","shell.execute_reply":"2024-03-04T13:24:31.234453Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"-----------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# Evaluation & Metrics","metadata":{}},{"cell_type":"markdown","source":"The below code evaluates the trained model on the test dataset:\n\n* It sets the model to evaluation mode.\n* It iterates through batches of test data from the test_loader.\n* For each batch, it moves the inputs and labels to the specified device (e.g., GPU).\n* It performs a forward pass through the model to obtain predictions.\n* It extracts predicted labels by selecting the index of the highest probability from the output.\n* It appends true and predicted labels to separate lists.\n* It counts the total number of samples processed.\n* It counts the number of correct predictions by comparing predicted labels with true labels.\n","metadata":{}},{"cell_type":"code","source":"correct, total = 0, 0\n\ntrue_labels, predicted_labels = [], []\n\n# Evaluate the model\nmodel.eval()\n\n# Iterate over the dataset or batches\nfor inputs, labels in test_loader:\n    # Forward pass\n    \n    inputs, labels = inputs.to(device), labels.to(device)\n    \n    outputs = model(inputs)\n    \n    # Get predicted labels\n    _, predicted = torch.max(outputs, 1)\n    \n    # Append true and predicted labels to lists\n    true_labels.extend(labels.cpu().numpy())\n    predicted_labels.extend(predicted.cpu().numpy())\n\n    # Count total number of samples\n    total += labels.size(0)\n    \n    # Count number of correct predictions\n    correct += (predicted == labels).sum().item()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T13:24:43.943224Z","iopub.execute_input":"2024-03-04T13:24:43.943608Z","iopub.status.idle":"2024-03-04T13:24:45.038398Z","shell.execute_reply.started":"2024-03-04T13:24:43.943581Z","shell.execute_reply":"2024-03-04T13:24:45.036919Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This below code calculates the accuracy of the model's predictions on the test dataset:\n\n* It divides the number of correct predictions (correct) by the total number of samples processed (total).\n* It prints the accuracy as a percentage.\n* Additionally, the code generates and plots the confusion matrix:\n* \n* It uses the true labels (true_labels) and predicted labels (predicted_labels) to compute the confusion matrix using the confusion_matrix function from scikit-learn.\n* It plots the confusion matrix as a heatmap using seaborn, with class names displayed on both axes.\n\nOverall, this code provides insights into the model's performance by quantifying its accuracy and visualizing its predictive behavior through the confusion matrix.","metadata":{}},{"cell_type":"code","source":"# Calculate accuracy\naccuracy = correct / total\nprint(f'Accuracy: {accuracy:.2f}')\n\n# Get the original class names\nclass_names = label_encoder.classes_\n\n# Generate confusion matrix\nconf_matrix = confusion_matrix(true_labels, predicted_labels)\n\n# Plot confusion matrix with class names\nplt.figure(figsize=(12, 6))\nsns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False, \n            xticklabels=class_names, yticklabels=class_names)\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T13:24:48.791882Z","iopub.execute_input":"2024-03-04T13:24:48.792817Z","iopub.status.idle":"2024-03-04T13:24:49.168666Z","shell.execute_reply.started":"2024-03-04T13:24:48.792773Z","shell.execute_reply":"2024-03-04T13:24:49.167772Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The below code generates a classification report and visualizes it as a heatmap:\n\n* It uses the classification_report function from scikit-learn to compute various metrics (precision, recall, F1-score, and support) for each class.\n* The target_names parameter specifies the class names to include in the report.\n* It converts the classification report dictionary into a DataFrame using pandas for easier visualization.\n* It plots the classification report DataFrame as a heatmap using seaborn, with metrics on the x-axis and classes on the y-axis.\n\nOverall, this code provides a comprehensive summary of the model's performance across different classes, allowing for a quick and intuitive assessment of its classification performance.","metadata":{}},{"cell_type":"code","source":"# Get classification report\nclass_report = classification_report(true_labels, predicted_labels, target_names=class_names, output_dict=True)\n\n# Convert classification report to a DataFrame\nclass_report_df = pd.DataFrame(class_report).transpose()\n\n# Plot heatmap\nplt.figure(figsize=(10, 6))\nsns.heatmap(class_report_df.iloc[:-1, :-1], annot=True, cmap='Blues', fmt='.2f')\nplt.title('Classification Report Heatmap')\nplt.xlabel('Metrics')\nplt.ylabel('Classes')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T13:24:55.025887Z","iopub.execute_input":"2024-03-04T13:24:55.026256Z","iopub.status.idle":"2024-03-04T13:24:55.44543Z","shell.execute_reply.started":"2024-03-04T13:24:55.026215Z","shell.execute_reply":"2024-03-04T13:24:55.444551Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"-------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# Real Video Test","metadata":{}},{"cell_type":"code","source":"\nimport os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport warnings\n\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfrom sklearn.decomposition import PCA\nfrom scipy.spatial import distance\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom IPython.display import display, HTML, Video\n\nfrom PIL import Image\nimport urllib.request\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset\n\n# Suppress FutureWarning\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T13:25:46.57799Z","iopub.execute_input":"2024-03-04T13:25:46.578709Z","iopub.status.idle":"2024-03-04T13:25:46.586537Z","shell.execute_reply.started":"2024-03-04T13:25:46.578675Z","shell.execute_reply":"2024-03-04T13:25:46.58556Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CNN2(nn.Module):\n    def __init__(self, num_classes):\n        super(CNN2, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.conv2 = nn.Conv2d(32, 32, kernel_size=5, padding=2)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.dropout1 = nn.Dropout2d(p=0.5)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.conv4 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n        self.dropout2 = nn.Dropout2d(p=0.5)\n        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.conv6 = nn.Conv2d(128, 128, kernel_size=5, padding=2)\n        self.dropout3 = nn.Dropout2d(p=0.5)\n        self.fc1 = nn.Linear(128 * 6 * 6, 256)\n        self.fc2 = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        x = self.dropout1(x)\n        x = F.relu(self.bn2(self.conv3(x)))\n        x = F.relu(self.conv4(x))\n        x = self.pool(x)\n        x = self.dropout2(x)\n        x = F.relu(self.bn3(self.conv5(x)))\n        x = F.relu(self.conv6(x))\n        x = self.pool(x)\n        x = self.dropout3(x)\n        x = x.view(-1, 128 * 6 * 6)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T13:25:51.665237Z","iopub.execute_input":"2024-03-04T13:25:51.665627Z","iopub.status.idle":"2024-03-04T13:25:51.678186Z","shell.execute_reply.started":"2024-03-04T13:25:51.665599Z","shell.execute_reply":"2024-03-04T13:25:51.677132Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The below code does:\n\n* The XML file containing the Haar cascade classifier for detecting frontal faces is downloaded from a specified URL using urllib.\n* The downloaded XML file is used to initialize the OpenCV CascadeClassifier object for face detection.\n* The pre-trained CNN model, trained for a specified number of classes, is instantiated.\n* The pre-trained weights of the model are loaded from the specified file path using torch.load(), ensuring that the model is loaded onto the CPU.\n* The model is set to evaluation mode using eval().","metadata":{}},{"cell_type":"code","source":"# URL of the XML file\nxml_url = 'https://github.com/opencv/opencv/raw/master/data/haarcascades/haarcascade_frontalface_default.xml'\n\nurllib.request.urlretrieve(xml_url, 'haarcascade_frontalface_default.xml')\nface_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n\n#Loading the model after training\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CNN2(7)\nmodel.load_state_dict(torch.load('/kaggle/input/test/pytorch/60-percentage-accuracy/1/my_model60.pth', map_location=torch.device(device)))\nmodel.to(device)\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T13:25:56.438359Z","iopub.execute_input":"2024-03-04T13:25:56.438732Z","iopub.status.idle":"2024-03-04T13:25:57.021469Z","shell.execute_reply.started":"2024-03-04T13:25:56.438704Z","shell.execute_reply":"2024-03-04T13:25:57.020073Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The below code does:\n\n* Loads a sample video from a dictionary of URLs representing different emotions.\n* Displays the video using IPython display, adjusting its width and height.\n* Opens the video for processing using OpenCV's VideoCapture.\n* Retrieves properties of the video: frame width, frame height, and frames per second (fps).\n* Creates a VideoWriter object to write processed frames into an output video file ('/kaggle/working/out.mp4') with codec 'mp4v'.\n* Prints the frame width, frame height, and frames per second of the input video.","metadata":{}},{"cell_type":"code","source":"some_sample_videos = {\"happy\": \"https://degenden-academy.s3.ap-south-1.amazonaws.com/webapp_assets/projects/dl/1.mp4\",\n                     \"angry\": \"https://degenden-academy.s3.ap-south-1.amazonaws.com/webapp_assets/projects/dl/2.mp4\",\n                     \"fear\": \"https://degenden-academy.s3.ap-south-1.amazonaws.com/webapp_assets/projects/dl/3.mp4\",\n                     \"surprise\": \"https://degenden-academy.s3.ap-south-1.amazonaws.com/webapp_assets/projects/dl/4.mp4\",\n                     \"sad\": \"https://degenden-academy.s3.ap-south-1.amazonaws.com/webapp_assets/projects/dl/5.mp4\",\n                     \"neutral\": \"https://degenden-academy.s3.ap-south-1.amazonaws.com/webapp_assets/projects/dl/6.mp4\",\n                     \"disgust\": \"https://degenden-academy.s3.ap-south-1.amazonaws.com/webapp_assets/projects/dl/7.mp4\"}\n\nvideo_path = some_sample_videos['happy']\ndisplay(Video(video_path, width= 500, height=300))\n\n# Open input video\noutput_video_path = '/kaggle/working/out.mp4'\n\nclass_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n\ncap = cv2.VideoCapture(video_path)\n\n# Get video properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = int(cap.get(cv2.CAP_PROP_FPS))\n\nprint(frame_width, frame_height, fps)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))","metadata":{"execution":{"iopub.status.busy":"2024-03-04T13:26:00.942911Z","iopub.execute_input":"2024-03-04T13:26:00.943533Z","iopub.status.idle":"2024-03-04T13:26:03.23826Z","shell.execute_reply.started":"2024-03-04T13:26:00.943501Z","shell.execute_reply":"2024-03-04T13:26:03.237289Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The below code processes each frame of the input video for face detection and emotion recognition:\n\n- It iterates through each frame of the input video captured by the video capture object (cap).\n- For each frame, it first converts the frame to grayscale for better face detection using the OpenCV cvtColor function.\n- It then utilizes the face cascade classifier (face_cascade) to detect faces in the grayscale frame.\n- Detected faces are cropped from the frame and preprocessed for emotion recognition:\n  - Each face region is converted to a PIL image and resized to 48x48 pixels.\n  - The resized image is converted to grayscale.\n  - The grayscale image is converted to a NumPy array and transformed into a PyTorch tensor.\n- The face image tensor is passed through the pre-trained CNN model to predict the emotion label.\n- Predicted emotion labels are used to annotate the frame with bounding boxes around detected faces and corresponding emotion labels using OpenCV's rectangle and putText functions.\n- The annotated frames are written to an output video file (out) using the VideoWriter object.\n- Once all frames are processed, the video capture object and the video writer object are released.\n- Finally, a message indicating the completion of video processing is printed.","metadata":{}},{"cell_type":"code","source":"# Process each frame of the input video\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Convert frame to grayscale for face detection\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n    # Detect faces in the frame\n    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n\n    # Process each detected face\n    for (x, y, w, h) in faces:\n        # Crop face region\n        face_img = frame[y:y+h, x:x+w]\n\n        # Convert face_img to PIL image\n        face_img_pil = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))\n        face_img_resized = face_img_pil.resize((48, 48))\n        face_img_gray = face_img_resized.convert('L')\n        \n        # Convert PIL image to NumPy array\n        face_img_np = np.array(face_img_gray)        \n        face_img_tensor = torch.tensor(face_img_np, dtype=torch.float).unsqueeze(0).unsqueeze(0).to(device)\n        \n        # Send face image tensor to the model\n        with torch.no_grad():\n            outputs = model(face_img_tensor)\n            \n        # Get predicted emotion label\n        _, predicted = torch.max(outputs, 1)\n        predicted_label = class_names[predicted.item()]\n                \n        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n        \n        # Draw the main text in orange\n        cv2.putText(frame, predicted_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 3.5, (0, 165, 255), 2)\n\n        # Draw slightly offset text to create a bold effect in orange\n        cv2.putText(frame, predicted_label, (x + 1, y - 9), cv2.FONT_HERSHEY_SIMPLEX, 3.5, (0, 165, 255), 2)\n        cv2.putText(frame, predicted_label, (x + 2, y - 8), cv2.FONT_HERSHEY_SIMPLEX, 3.5, (0, 165, 255), 2)\n\n    out.write(frame)\n    \n    \n# Release everything if job is finished\ncap.release()\nout.release()\n\nprint(\"Video is Done\")","metadata":{"execution":{"iopub.status.busy":"2024-03-04T13:26:07.560979Z","iopub.execute_input":"2024-03-04T13:26:07.561694Z","iopub.status.idle":"2024-03-04T13:29:12.966518Z","shell.execute_reply.started":"2024-03-04T13:26:07.56166Z","shell.execute_reply":"2024-03-04T13:29:12.96555Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"------------------------","metadata":{}},{"cell_type":"markdown","source":"# LIVE Feed Video Test\n\nThe below code works only on local machine, not on kaggle environment here.","metadata":{}},{"cell_type":"code","source":"import urllib.request\nimport cv2\n\nfrom PIL import Image\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset","metadata":{"execution":{"iopub.status.busy":"2024-03-04T13:31:09.021711Z","iopub.execute_input":"2024-03-04T13:31:09.022098Z","iopub.status.idle":"2024-03-04T13:31:09.028234Z","shell.execute_reply.started":"2024-03-04T13:31:09.022068Z","shell.execute_reply":"2024-03-04T13:31:09.027274Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CNN2(nn.Module):\n    def __init__(self, num_classes):\n        super(CNN2, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.conv2 = nn.Conv2d(32, 32, kernel_size=5, padding=2)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.dropout1 = nn.Dropout2d(p=0.5)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.conv4 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n        self.dropout2 = nn.Dropout2d(p=0.5)\n        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.conv6 = nn.Conv2d(128, 128, kernel_size=5, padding=2)\n        self.dropout3 = nn.Dropout2d(p=0.5)\n        self.fc1 = nn.Linear(128 * 6 * 6, 256)\n        self.fc2 = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        x = self.dropout1(x)\n        x = F.relu(self.bn2(self.conv3(x)))\n        x = F.relu(self.conv4(x))\n        x = self.pool(x)\n        x = self.dropout2(x)\n        x = F.relu(self.bn3(self.conv5(x)))\n        x = F.relu(self.conv6(x))\n        x = self.pool(x)\n        x = self.dropout3(x)\n        x = x.view(-1, 128 * 6 * 6)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-03-04T13:31:11.504347Z","iopub.execute_input":"2024-03-04T13:31:11.504707Z","iopub.status.idle":"2024-03-04T13:31:11.519044Z","shell.execute_reply.started":"2024-03-04T13:31:11.50468Z","shell.execute_reply":"2024-03-04T13:31:11.518164Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_live_video(faceCascade):\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    model = CNN2(7)\n    model.load_state_dict(torch.load('/kaggle/input/test/pytorch/60-percentage-accuracy/1/my_model60.pth', map_location=torch.device(device)))\n\n    print(\"Model has been loaded\")\n\n    class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n\n    cap = cv2.VideoCapture(0)\n\n    if not cap.isOpened():\n        print(\"Error: Failed to open camera.\")\n        return\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            print(\"Error: Failed to capture frame.\")\n            break\n\n        # Convert frame to grayscale for face detection\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(30, 30))\n\n        for (x, y, w, h) in faces:\n\n            # Crop face region\n            face_img = frame[y:y+h, x:x+w]\n\n            # Convert face_img to PIL image\n            face_img_pil = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))\n            face_img_resized = face_img_pil.resize((48, 48))\n            face_img_gray = face_img_resized.convert('L')\n\n\n            # Convert PIL image to NumPy array\n            face_img_np = np.array(face_img_gray)        \n            face_img_tensor = torch.tensor(face_img_np, dtype=torch.float).unsqueeze(0).unsqueeze(0).to('cuda' if torch.cuda.is_available() else 'cpu')\n\n            # Send face image tensor to the model\n            with torch.no_grad():\n                outputs = model(face_img_tensor)\n                \n            # Get predicted emotion label\n            _, predicted = torch.max(outputs, 1)\n            predicted_label = class_names[predicted.item()]\n\n            # Draw rectangle around detected face and display predicted emotion label\n            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n            \n            # Draw the main text in orange\n            cv2.putText(frame, predicted_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 3.5, (0, 165, 255), 2)\n\n            # Draw slightly offset text to create a bold effect in orange\n            cv2.putText(frame, predicted_label, (x + 1, y - 9), cv2.FONT_HERSHEY_SIMPLEX, 3.5, (0, 165, 255), 2)\n            cv2.putText(frame, predicted_label, (x + 2, y - 8), cv2.FONT_HERSHEY_SIMPLEX, 3.5, (0, 165, 255), 2)\n\n        \n        print(frame.shape)\n        \n        cv2.imshow('FRAME',frame)\n\n\n        key = cv2.waitKey(20)\n        if key == 27: # exit on ESC\n            break\n\n    vc.release()\n    cv2.destroyAllWindows()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T13:31:14.462818Z","iopub.execute_input":"2024-03-04T13:31:14.46361Z","iopub.status.idle":"2024-03-04T13:31:14.478637Z","shell.execute_reply.started":"2024-03-04T13:31:14.463572Z","shell.execute_reply":"2024-03-04T13:31:14.477672Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xml_url = 'https://github.com/opencv/opencv/raw/master/data/haarcascades/haarcascade_frontalface_default.xml'\nurllib.request.urlretrieve(xml_url, 'haarcascade_frontalface_default.xml')\n\nface_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\nprocess_live_video(face_cascade)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T13:31:18.652705Z","iopub.execute_input":"2024-03-04T13:31:18.65355Z","iopub.status.idle":"2024-03-04T13:31:19.095466Z","shell.execute_reply.started":"2024-03-04T13:31:18.653514Z","shell.execute_reply":"2024-03-04T13:31:19.094539Z"},"trusted":true},"outputs":[],"execution_count":null}]}