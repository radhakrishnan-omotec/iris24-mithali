# -*- coding: utf-8 -*-
"""Mitali_rao_project_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/radhakrishnan-omotec/iris24-mithali/blob/main/Mitali_rao_project_1.ipynb

###**Project Title**
#The Impact of Music Genres on Study Efficiency: Analyzing Distraction through Eye and Facial Movement Tracking thru Biometrics

### Author
**Mitali Rao**

Environment Setup:

    Install necessary libraries: Run the following command if needed:
"""

!pip install opencv-python tensorflow librosa statsmodels scikit-learn

"""###Python notebook Methodologuy outline:

Implement in Google Colab for your project titled "The Impact of Music Genres on Study Efficiency: Analyzing Distraction through Eye and Facial Movement Tracking through Biometrics." This notebook incorporates data collection, analysis, modeling, and visualization techniques required for your study.

#### Step 1: Setup
#### Importing necessary libraries
"""

# Notebook Title: The Impact of Music Genres on Study Efficiency: Analyzing Distraction through Biometrics

# Step 1: Setup
# Importing necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
import cv2
import librosa
from scipy.signal import find_peaks
import time
from statsmodels.tsa.ar_model import AutoReg

"""#### Step 2: Application Development
#### Loading Music
"""

# Step 2: Application Development
# Loading Music
music_genres = ['Classical', 'Jazz', 'Pop', 'Electronic']
selected_genre = input(f"Select a music genre from {music_genres}: ")

"""####Step 3: Biometric Data Collection and Preprocessing

**Enhancements for Step 3: Biometric Data Collection and Preprocessing**

    **Dataset:**
    We will use a pre-existing dataset (e.g., FER2013, a dataset for facial expression recognition with labels like 'happy', 'sad', 'angry', etc.).
    Preprocessing: Load the dataset, preprocess images (resize, normalize), and split them into training and testing sets.
    Model Training: We'll modify the code to train the CNN model using the loaded facial expression dataset.

**Explanation:**

    load_facial_expression_dataset: This function reads images from a dataset directory, preprocesses them by resizing to 48x48 (standard size for FER2013), normalizes them, and converts labels to categorical format.
    
    ImageDataGenerator: This allows you to augment the training data by applying random transformations like rotations, zooms, and flips, which can help the model generalize better.
"""

# Step 3: Biometric Data Collection and Preprocessing
# Instead of capturing video, we'll load a facial expression dataset (e.g., FER2013).

from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from tensorflow.keras.applications.vgg16 import preprocess_input
import os
import cv2

# Define the function to load and preprocess the facial expression dataset
def load_facial_expression_dataset(dataset_path):
    image_size = (48, 48)  # Image size for FER2013
    images = []
    labels = []

    # Assuming FER2013 dataset structure (LATER replace with your dataset path and structure)
    for emotion in os.listdir(dataset_path):
        emotion_folder = os.path.join(dataset_path, emotion)
        if os.path.isdir(emotion_folder):
            for image_file in os.listdir(emotion_folder):
                image_path = os.path.join(emotion_folder, image_file)
                # Read image in grayscale
                img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
                # Resize the image
                img = cv2.resize(img, image_size)
                images.append(img)
                # The emotion label is the folder name
                labels.append(emotion)

    # Convert to numpy arrays
    images = np.array(images).reshape(-1, 48, 48, 1)  # Add channel dimension for grayscale
    labels = np.array(labels)

    # Normalize the images
    images = images / 255.0

    # Convert labels to numerical categories
    unique_labels = sorted(list(set(labels)))
    label_to_index = {label: idx for idx, label in enumerate(unique_labels)}
    labels = np.array([label_to_index[label] for label in labels])

    return images, labels, label_to_index

# Path to your facial expression dataset
dataset_path = '/facial/expression/dataset'  # e.g., FER2013 directory
images, labels, label_to_index = load_facial_expression_dataset(dataset_path)

# Convert labels to categorical (for classification)
labels_categorical = to_categorical(labels)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(images, labels_categorical, test_size=0.2, random_state=42)

print(f"Training set shape: {X_train.shape}")
print(f"Test set shape: {X_test.shape}")

# Data Augmentation (optional for improving training)
datagen = ImageDataGenerator(rotation_range=10, zoom_range=0.1, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)
datagen.fit(X_train)

"""#### Step 4: Eye Movement Analysis (Focus Assessment)
#### Defining a function to quantify focus or distraction from eye movement data
"""

# Step 4: Eye Movement Analysis (Focus Assessment)
# Defining a function to quantify focus or distraction from eye movement data
def analyze_eye_movement(data, window_size):
    focus_variability = []
    for i in range(0, len(data), window_size):
        window_data = data[i:i + window_size]
        variability = np.std(window_data)
        focus_variability.append(variability)
    return focus_variability

# Applying Eye Movement Analysis
focus_variability = analyze_eye_movement(eye_movement_data, 50)
plt.plot(focus_variability)
plt.title('Eye Movement Variability (Focus Assessment)')
plt.show()

"""### Step 5: Facial Expression Analysis (Emotional Response Assessment)
### Defining a function to analyze facial expression changes over time
"""

# Step 5: Facial Expression Analysis (Emotional Response Assessment)
# Defining a function to analyze facial expression changes over time
def analyze_facial_expressions(data, window_size):
    expression_variability = []
    for i in range(0, len(data), window_size):
        window_data = data[i:i + window_size]
        variability = np.std(window_data)
        expression_variability.append(variability)
    return expression_variability

# Applying Facial Expression Analysis
facial_expression_variability = analyze_facial_expressions(facial_expression_data, 50)
plt.plot(facial_expression_variability)
plt.title('Facial Expression Variability (Emotional Response)')
plt.show()

"""### Step 6: Statistical and Predictive Modeling
### Statistical and Predictive Modeling using CNN for facial expression recognition

**Explanation of the CNN:**

    Model Architecture:
        Three convolutional layers (Conv2D) with ReLU activation and max pooling layers (MaxPooling2D).
        A fully connected layer followed by a dropout layer (Dropout) to prevent overfitting.
        The output layer uses softmax activation, suitable for multi-class classification (emotion categories).
        
    Training: The model is trained on the augmented dataset using Adam optimizer and categorical crossentropy loss. Training and validation accuracy are plotted.
"""

# Step 6: Statistical and Predictive Modeling
# Statistical and Predictive Modeling using CNN for facial expression recognition

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# Define the CNN model for facial expression recognition
def build_cnn_model(input_shape, num_classes):
    model = Sequential()

    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Conv2D(128, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))

    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    return model

# Build and compile the model
input_shape = (48, 48, 1)  # FER2013 image dimensions (48x48 grayscale)
num_classes = len(label_to_index)  # Number of emotion categories
model = build_cnn_model(input_shape, num_classes)

# Train the model
batch_size = 64
epochs = 30

history = model.fit(datagen.flow(X_train, y_train, batch_size=batch_size), validation_data=(X_test, y_test), epochs=epochs)

# Plot training and validation accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""### Evaluate the model on test set and get predictions

**Breakdown of Features:**

    Classification Report: Provides precision, recall, F1-score, and support for each class (emotion label).
        Precision: How many selected items are relevant.
        Recall: How many relevant items are selected.
        F1-score: Harmonic mean of precision and recall.


    Confusion Matrix: Helps visualize the performance of the classification model by showing where the model gets confused between classes. It's presented as a heatmap for better readability.


    Accuracy Score: A common metric in classification tasks that shows the overall percentage of correctly classified samples.
    

    Scatter Plot: Shows how well the predicted labels match the actual labels. While scatter plots are often used in regression, they can still give a useful visual comparison in classification tasks, especially when you want to see where the model's predictions are concentrated.
"""

# Setup of Evaluate the model on test set and get predictions
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Evaluate the model on test set and get predictions
y_pred_prob = model.predict(X_test)  # Probabilities from the model
y_pred = np.argmax(y_pred_prob, axis=1)  # Convert probabilities to class labels
y_test_labels = np.argmax(y_test, axis=1)  # Convert one-hot encoded y_test to class labels

# Classification Report
print("Classification Report:")
print(classification_report(y_test_labels, y_pred, target_names=label_to_index.keys()))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test_labels, y_pred)

# Plot Confusion Matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_to_index.keys(), yticklabels=label_to_index.keys())
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Calculate R^2 Score Equivalent for Classification (Accuracy)
test_accuracy = accuracy_score(y_test_labels, y_pred)
print(f"Test Accuracy: {test_accuracy*100:.2f}%")

# Scatter Plot: Actual vs Predicted Classes
plt.figure(figsize=(6, 6))
plt.scatter(y_test_labels, y_pred, alpha=0.5)
plt.title('Actual vs Predicted Emotion Labels')
plt.xlabel('Actual Emotion Label')
plt.ylabel('Predicted Emotion Label')
plt.grid(True)
plt.show()

"""**Additional Enhancements:**

**Precision, Recall, and F1-scores** give deeper insights into your model's performance, especially in cases of imbalanced datasets where accuracy alone might be misleading.
**Confusion matrix heatmap** visually pinpoints which emotions the model often confuses, helping you understand the model's strengths and weaknesses.

This enhanced evaluation provides more detailed insights into how well your CNN is performing in terms of classification and helps visualize the comparison between actual and predicted classes. You can further adjust these metrics and visualizations based on your project's specific needs.

**Explanation of the Code**:

    Precision, Recall, and F1-scores:
        Precision: Measures how many of the predicted labels were actually correct for each class.
        Recall: Measures how many of the actual labels were correctly predicted for each class.
        F1-score: Harmonic mean of precision and recall, giving an overall score for each class.
        The average='weighted' option calculates these metrics across all classes, weighted by the number of samples in each class (useful if the dataset is imbalanced).

    Confusion Matrix Heatmap:
        A visual representation of the confusion matrix using a heatmap, where the diagonal values represent the correct predictions, and off-diagonal values show where the model made incorrect predictions.
        The x-axis represents the predicted class labels, while the y-axis represents the true class labels.

    Classification Report:
        A detailed report that includes precision, recall, and F1-score for each class, as well as support (number of samples) for each class.

    Accuracy Score:
        The overall accuracy of the model is displayed, showing the percentage of correct predictions across the entire test set.

    Scatter Plot:
        This plot compares the actual vs predicted class labels, providing a visual sense of how well the model is performing. Ideally, the points should form a diagonal line if the predictions match the actual labels perfectly.
"""

#Additional Enhancements
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# The model has been trained and now you are using X_test to evaluate it.

# Predicting the probabilities for the test set
y_pred_prob = model.predict(X_test)  # Probabilities from the CNN model

# Converting probabilities to predicted class labels
y_pred = np.argmax(y_pred_prob, axis=1)  # Convert to predicted class labels
y_test_labels = np.argmax(y_test, axis=1)  # Convert one-hot encoded test labels to original class labels

# 1. Precision, Recall, F1-scores

# Calculate precision, recall, and F1-score for each class
precision = precision_score(y_test_labels, y_pred, average=None)  # Precision per class
recall = recall_score(y_test_labels, y_pred, average=None)  # Recall per class
f1 = f1_score(y_test_labels, y_pred, average=None)  # F1-score per class

# Printing Precision, Recall, and F1-score for each class
for i, label in enumerate(label_to_index.keys()):  # Assuming label_to_index is the dictionary for emotion labels
    print(f"Class: {label}")
    print(f"  Precision: {precision[i]:.2f}")
    print(f"  Recall:    {recall[i]:.2f}")
    print(f"  F1-Score:  {f1[i]:.2f}")
    print()

# Overall precision, recall, F1-score (averaged over all classes)
overall_precision = precision_score(y_test_labels, y_pred, average='weighted')
overall_recall = recall_score(y_test_labels, y_pred, average='weighted')
overall_f1 = f1_score(y_test_labels, y_pred, average='weighted')

print(f"Overall Precision: {overall_precision:.2f}")
print(f"Overall Recall:    {overall_recall:.2f}")
print(f"Overall F1-Score:  {overall_f1:.2f}")
print()

# 2. Confusion Matrix Heatmap

# Generate the confusion matrix
conf_matrix = confusion_matrix(y_test_labels, y_pred)

# Plot the confusion matrix using Seaborn heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=label_to_index.keys(), yticklabels=label_to_index.keys())
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# 3. Classification Report

# Generate a detailed classification report
class_report = classification_report(y_test_labels, y_pred, target_names=label_to_index.keys())
print("Classification Report:")
print(class_report)

# 4. Accuracy Score

# Calculate accuracy
test_accuracy = accuracy_score(y_test_labels, y_pred)
print(f"Test Accuracy: {test_accuracy*100:.2f}%")

# 5. Scatter Plot: Actual vs Predicted

plt.figure(figsize=(6, 6))
plt.scatter(y_test_labels, y_pred, alpha=0.5, edgecolor='k')
plt.title('Actual vs Predicted Emotion Labels')
plt.xlabel('Actual Emotion Label')
plt.ylabel('Predicted Emotion Label')
plt.grid(True)
plt.show()

"""### More Evaluations for Multiclass level

Evaluating your CNN model using Cohen's Kappa and Matthews Correlation Coefficient (MCC) in addition to the previous metrics. These metrics are particularly useful for evaluating multi-class models and imbalanced datasets.
"""

from sklearn.metrics import cohen_kappa_score, matthews_corrcoef

# The model has been trained and y_pred and y_test_labels are available

# Predicting probabilities for the test set
y_pred_prob = model.predict(X_test)  # Probabilities from the CNN model

# Converting probabilities to predicted class labels
y_pred = np.argmax(y_pred_prob, axis=1)  # Convert to predicted class labels
y_test_labels = np.argmax(y_test, axis=1)  # Convert one-hot encoded test labels to original class labels

# 6. Cohen's Kappa

# Calculate Cohen's Kappa Score
kappa_score = cohen_kappa_score(y_test_labels, y_pred)
print(f"Cohen's Kappa Score: {kappa_score:.2f}")

# 7. Matthews Correlation Coefficient (MCC)

# Calculate MCC
mcc = matthews_corrcoef(y_test_labels, y_pred)
print(f"Matthews Correlation Coefficient (MCC): {mcc:.2f}")


plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""---
----
"""